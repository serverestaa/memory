{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0DcgIBAPBS-",
        "outputId": "e6946ebe-f6f0-49b1-d53a-094588693a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faM-T4pHQUNI",
        "outputId": "ddfa4fb8-e806-4d8a-e7eb-8c8b7135e141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Chinese subjects...\n",
            "Loading and processing Chinese subject 1...\n",
            "  Data shape: (996, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/996\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/996\n",
            "Loading and processing Chinese subject 2...\n",
            "  Data shape: (884, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/884\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/884\n",
            "Loading and processing Chinese subject 3...\n",
            "  Data shape: (1291, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/1291\n",
            "  Filtering trial 1000/1291\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/1291\n",
            "  Normalizing trial 1000/1291\n",
            "Loading and processing Chinese subject 4...\n",
            "  Data shape: (908, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/908\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/908\n",
            "Loading and processing Chinese subject 5...\n",
            "  Data shape: (882, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/882\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/882\n",
            "Loading and processing Chinese subject 6...\n",
            "  Data shape: (904, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/904\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/904\n",
            "Loading and processing Chinese subject 7...\n",
            "  Data shape: (942, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/942\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/942\n",
            "Loading and processing Chinese subject 8...\n",
            "  Data shape: (1453, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/1453\n",
            "  Filtering trial 1000/1453\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/1453\n",
            "  Normalizing trial 1000/1453\n",
            "Loading and processing Chinese subject 9...\n",
            "  Data shape: (1059, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/1059\n",
            "  Filtering trial 1000/1059\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/1059\n",
            "  Normalizing trial 1000/1059\n",
            "Processing Korean subjects...\n",
            "Loading and processing Korean subject 1...\n",
            "  Data shape: (2536, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2536\n",
            "  Filtering trial 1000/2536\n",
            "  Filtering trial 2000/2536\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2536\n",
            "  Normalizing trial 1000/2536\n",
            "  Normalizing trial 2000/2536\n",
            "Loading and processing Korean subject 2...\n",
            "  Data shape: (2873, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2873\n",
            "  Filtering trial 1000/2873\n",
            "  Filtering trial 2000/2873\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2873\n",
            "  Normalizing trial 1000/2873\n",
            "  Normalizing trial 2000/2873\n",
            "Loading and processing Korean subject 3...\n",
            "  Data shape: (2486, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2486\n",
            "  Filtering trial 1000/2486\n",
            "  Filtering trial 2000/2486\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2486\n",
            "  Normalizing trial 1000/2486\n",
            "  Normalizing trial 2000/2486\n",
            "Loading and processing Korean subject 4...\n",
            "  Data shape: (2660, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2660\n",
            "  Filtering trial 1000/2660\n",
            "  Filtering trial 2000/2660\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2660\n",
            "  Normalizing trial 1000/2660\n",
            "  Normalizing trial 2000/2660\n",
            "Loading and processing Korean subject 5...\n",
            "  Data shape: (2180, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2180\n",
            "  Filtering trial 1000/2180\n",
            "  Filtering trial 2000/2180\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2180\n",
            "  Normalizing trial 1000/2180\n",
            "  Normalizing trial 2000/2180\n",
            "Loading and processing Korean subject 6...\n",
            "  Data shape: (2805, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2805\n",
            "  Filtering trial 1000/2805\n",
            "  Filtering trial 2000/2805\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2805\n",
            "  Normalizing trial 1000/2805\n",
            "  Normalizing trial 2000/2805\n",
            "Loading and processing Korean subject 7...\n",
            "  Data shape: (3606, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/3606\n",
            "  Filtering trial 1000/3606\n",
            "  Filtering trial 2000/3606\n",
            "  Filtering trial 3000/3606\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/3606\n",
            "  Normalizing trial 1000/3606\n",
            "  Normalizing trial 2000/3606\n",
            "  Normalizing trial 3000/3606\n",
            "Loading and processing Korean subject 8...\n",
            "  Data shape: (2983, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2983\n",
            "  Filtering trial 1000/2983\n",
            "  Filtering trial 2000/2983\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2983\n",
            "  Normalizing trial 1000/2983\n",
            "  Normalizing trial 2000/2983\n",
            "Loading and processing Korean subject 9...\n",
            "  Data shape: (2947, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2947\n",
            "  Filtering trial 1000/2947\n",
            "  Filtering trial 2000/2947\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2947\n",
            "  Normalizing trial 1000/2947\n",
            "  Normalizing trial 2000/2947\n",
            "Loading and processing Korean subject 10...\n",
            "  Data shape: (3405, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/3405\n",
            "  Filtering trial 1000/3405\n",
            "  Filtering trial 2000/3405\n",
            "  Filtering trial 3000/3405\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/3405\n",
            "  Normalizing trial 1000/3405\n",
            "  Normalizing trial 2000/3405\n",
            "  Normalizing trial 3000/3405\n",
            "Loading and processing Korean subject 11...\n",
            "  Data shape: (3008, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/3008\n",
            "  Filtering trial 1000/3008\n",
            "  Filtering trial 2000/3008\n",
            "  Filtering trial 3000/3008\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/3008\n",
            "  Normalizing trial 1000/3008\n",
            "  Normalizing trial 2000/3008\n",
            "  Normalizing trial 3000/3008\n",
            "Loading and processing Korean subject 12...\n",
            "  Data shape: (3098, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/3098\n",
            "  Filtering trial 1000/3098\n",
            "  Filtering trial 2000/3098\n",
            "  Filtering trial 3000/3098\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/3098\n",
            "  Normalizing trial 1000/3098\n",
            "  Normalizing trial 2000/3098\n",
            "  Normalizing trial 3000/3098\n",
            "Loading and processing Korean subject 13...\n",
            "  Data shape: (3897, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/3897\n",
            "  Filtering trial 1000/3897\n",
            "  Filtering trial 2000/3897\n",
            "  Filtering trial 3000/3897\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/3897\n",
            "  Normalizing trial 1000/3897\n",
            "  Normalizing trial 2000/3897\n",
            "  Normalizing trial 3000/3897\n",
            "Loading and processing Korean subject 14...\n",
            "  Data shape: (2637, 55, 100)\n",
            "  Applying spatial filter...\n",
            "  Applying bandpass filter...\n",
            "  Filtering trial 0/2637\n",
            "  Filtering trial 1000/2637\n",
            "  Filtering trial 2000/2637\n",
            "  Applying memory normalization...\n",
            "  Normalizing trial 0/2637\n",
            "  Normalizing trial 1000/2637\n",
            "  Normalizing trial 2000/2637\n",
            "Combining all processed data...\n",
            "Shape of original data: (50440, 55, 100)\n",
            "Original label distribution: [32197 18243]\n",
            "\n",
            "--- Label distribution BEFORE balancing ---\n",
            "Training set: [25758 14594]\n",
            "Validation set: [3219 1825]\n",
            "Testing set: [3220 1824]\n",
            "\n",
            "Applying SMOTE to the training set...\n",
            "\n",
            "--- Final Data Distribution & Shapes ---\n",
            "Training set label distribution AFTER balancing: [25758 25758]\n",
            "Validation set label distribution (Unchanged): [3219 1825]\n",
            "Testing set label distribution (Unchanged): [3220 1824]\n",
            "\n",
            "Balanced training set shape: (51516, 55, 100)\n",
            "Validation set shape: (5044, 55, 100)\n",
            "Testing set shape: (5044, 55, 100)\n"
          ]
        }
      ],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.io import loadmat\n",
        "from scipy import signal\n",
        "import gc  # Garbage collection\n",
        "\n",
        "def apply_spatial_filter(data):\n",
        "    \"\"\"Apply spatial filtering (CAR) efficiently\"\"\"\n",
        "    channel_mean = np.mean(data, axis=1, keepdims=True)\n",
        "    return data - channel_mean\n",
        "\n",
        "def apply_memory_normalization(data):\n",
        "    \"\"\"Apply optimal normalization for memory data\"\"\"\n",
        "    # Per-trial robust normalization to preserve memory-related amplitude differences\n",
        "    n_trials, n_channels, n_times = data.shape\n",
        "    normalized_data = np.zeros_like(data)\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"  Normalizing trial {i}/{n_trials}\")\n",
        "\n",
        "        trial_data = data[i]  # Shape: (channels, time)\n",
        "\n",
        "        # Robust normalization per trial (less sensitive to outliers than z-score)\n",
        "        trial_median = np.median(trial_data)\n",
        "        trial_mad = np.median(np.abs(trial_data - trial_median))  # Median Absolute Deviation\n",
        "\n",
        "        # Normalize: (data - median) / (1.4826 * MAD)\n",
        "        # 1.4826 makes MAD equivalent to std for normal distribution\n",
        "        if trial_mad > 0:\n",
        "            normalized_data[i] = (trial_data - trial_median) / (1.4826 * trial_mad)\n",
        "        else:\n",
        "            normalized_data[i] = trial_data - trial_median\n",
        "\n",
        "    return normalized_data\n",
        "\n",
        "def apply_bandpass_filter_safe(data, fs=100, low_freq=0.5, high_freq=45.0, order=4):\n",
        "    \"\"\"Apply bandpass filter with memory management\"\"\"\n",
        "    # Create filter coefficients\n",
        "    nyquist = fs / 2\n",
        "    low_norm = low_freq / nyquist\n",
        "    high_norm = high_freq / nyquist\n",
        "    b, a = signal.butter(order, [low_norm, high_norm], btype='band')\n",
        "\n",
        "    # Process trial by trial to save memory\n",
        "    n_trials, n_channels, n_times = data.shape\n",
        "    data_filtered = np.zeros_like(data, dtype=np.float32)  # Use float32 to save memory\n",
        "\n",
        "    for i in range(n_trials):\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"  Filtering trial {i}/{n_trials}\")\n",
        "        data_filtered[i] = signal.filtfilt(b, a, data[i], axis=1)\n",
        "\n",
        "    return data_filtered\n",
        "\n",
        "# Setup channel lists\n",
        "ch_ch = list(range(63))\n",
        "g_ch = list(range(61))\n",
        "ch_rem = [0, 2, 6, 8, 29, 31, 51, 55]\n",
        "g_rem = [46, 49, 56, 58, 59, 60]\n",
        "\n",
        "for i in ch_rem:\n",
        "    ch_ch.remove(i)\n",
        "for j in g_rem:\n",
        "    g_ch.remove(j)\n",
        "\n",
        "# Process subjects one by one to manage memory\n",
        "processed_data = []\n",
        "processed_labels = []\n",
        "\n",
        "print(\"Processing Chinese subjects...\")\n",
        "for subj in range(1, 10):\n",
        "    file_path = f\"/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/data_ch/ASK/sbj_{subj}.mat\"\n",
        "    try:\n",
        "        print(f\"Loading and processing Chinese subject {subj}...\")\n",
        "        mat_data = loadmat(file_path)\n",
        "        X = np.array(mat_data['x_post'])[:-1, ch_ch, :].astype(np.float32)  # Use float32\n",
        "        y = np.array(mat_data['label_next_ind'])[0, :]\n",
        "        X = np.transpose(X)\n",
        "\n",
        "        print(f\"  Data shape: {X.shape}\")\n",
        "\n",
        "        # Apply spatial filtering\n",
        "        print(\"  Applying spatial filter...\")\n",
        "        X_spatial = apply_spatial_filter(X)\n",
        "\n",
        "        # Apply bandpass filtering\n",
        "        print(\"  Applying bandpass filter...\")\n",
        "        X_filtered = apply_bandpass_filter_safe(X_spatial)\n",
        "\n",
        "        # Apply memory-optimized normalization\n",
        "        print(\"  Applying memory normalization...\")\n",
        "        X_normalized = apply_memory_normalization(X_filtered)\n",
        "\n",
        "        processed_data.append(X_normalized)\n",
        "        processed_labels.append(y)\n",
        "\n",
        "        # Clean up memory\n",
        "        del X, X_spatial, X_filtered, X_normalized, mat_data\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file {file_path}: {e}\")\n",
        "\n",
        "print(\"Processing Korean subjects...\")\n",
        "for subj in range(1, 15):\n",
        "    file_path = f'/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/data/ASK/sbj_{subj}.mat'\n",
        "    try:\n",
        "        print(f\"Loading and processing Korean subject {subj}...\")\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            X = np.array(f['x_post'])[:, g_ch, :].astype(np.float32)  # Use float32\n",
        "            y = np.array(f['label_next_ind'])[:, 0]\n",
        "\n",
        "        print(f\"  Data shape: {X.shape}\")\n",
        "\n",
        "        # Apply spatial filtering\n",
        "        print(\"  Applying spatial filter...\")\n",
        "        X_spatial = apply_spatial_filter(X)\n",
        "\n",
        "        # Apply bandpass filtering\n",
        "        print(\"  Applying bandpass filter...\")\n",
        "        X_filtered = apply_bandpass_filter_safe(X_spatial)\n",
        "\n",
        "        # Apply memory-optimized normalization\n",
        "        print(\"  Applying memory normalization...\")\n",
        "        X_normalized = apply_memory_normalization(X_filtered)\n",
        "\n",
        "        processed_data.append(X_normalized)\n",
        "        processed_labels.append(y)\n",
        "\n",
        "        # Clean up memory\n",
        "        del X, X_spatial, X_filtered, X_normalized\n",
        "        gc.collect()\n",
        "\n",
        "    except OSError as e:\n",
        "        print(f\"Error loading file {file_path}: {e}\")\n",
        "\n",
        "# Now concatenate all processed data\n",
        "print(\"Combining all processed data...\")\n",
        "data = np.concatenate(processed_data, axis=0)\n",
        "labels = np.concatenate(processed_labels, axis=0)\n",
        "\n",
        "# Clean up\n",
        "del processed_data, processed_labels\n",
        "gc.collect()\n",
        "\n",
        "print(\"Shape of original data:\", data.shape)\n",
        "print(\"Original label distribution:\", np.bincount(labels.astype(int)))\n",
        "\n",
        "# --- Step 1: Split data BEFORE any resampling ---\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "# Clean up\n",
        "del data, X_temp, y_temp\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n--- Label distribution BEFORE balancing ---\")\n",
        "print(\"Training set:\", np.bincount(y_train.astype(int)))\n",
        "print(\"Validation set:\", np.bincount(y_val.astype(int)))\n",
        "print(\"Testing set:\", np.bincount(y_test.astype(int)))\n",
        "\n",
        "\n",
        "# --- Step 2: Apply SMOTE ONLY to the training data ---\n",
        "print(\"\\nApplying SMOTE to the training set...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Reshape training data for the sampler (from 3D to 2D)\n",
        "train_shape = X_train.shape\n",
        "X_train_reshaped = X_train.reshape(train_shape[0], -1)\n",
        "\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_reshaped, y_train)\n",
        "\n",
        "# Reshape the balanced training data back to its original 3D format\n",
        "X_train_resampled = X_train_resampled.reshape(-1, train_shape[1], train_shape[2])\n",
        "\n",
        "print(\"\\n--- Final Data Distribution & Shapes ---\")\n",
        "print(\"Training set label distribution AFTER balancing:\", np.bincount(y_train_resampled.astype(int)))\n",
        "print(\"Validation set label distribution (Unchanged):\", np.bincount(y_val.astype(int)))\n",
        "print(\"Testing set label distribution (Unchanged):\", np.bincount(y_test.astype(int)))\n",
        "\n",
        "print(\"\\nBalanced training set shape:\", X_train_resampled.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "print(\"Testing set shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja cmake\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install --no-build-isolation mamba-ssm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh6trS4JLz6c",
        "outputId": "9624c5b6-5046-494b-c580-67a9501856f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.12/dist-packages (3.31.6)\n",
            "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/180.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.13.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (2.8.0+cu126)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (1.13.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (4.56.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (25.0)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.11.1.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2025.8.3)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.5-cp312-cp312-linux_x86_64.whl size=532566033 sha256=c8b65fcabfb49a94456c9971619007218e4073f19a84fb6b3894f33d43bee4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/55/c4/85b634055d6a9b599d27f5cbeacf353c6c532d8e2d8769960b\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from mamba_ssm import Mamba\n",
        "\n",
        "class ConvEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.norm = nn.BatchNorm1d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # [B, T, C] → [B, C, T]\n",
        "        x = self.conv(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.activation(x)\n",
        "        x = x.transpose(1, 2)  # [B, C, T] → [B, T, C]\n",
        "        return x\n",
        "\n",
        "\n",
        "class STMambaNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(STMambaNet, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.kernel_sizes = [5, 9]\n",
        "        self.conv_channels = [4, 4]\n",
        "\n",
        "        self.temp_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_ch, kernel_size=(1, k), padding=(0, k // 2))\n",
        "            for k, out_ch in zip(self.kernel_sizes, self.conv_channels)\n",
        "        ])\n",
        "\n",
        "        self.total_conv_channels = sum(self.conv_channels)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(self.total_conv_channels)\n",
        "        self.spatial_conv = nn.Conv2d(in_channels=self.total_conv_channels, out_channels=self.total_conv_channels, kernel_size=(1, 1))\n",
        "        self.batch_norm2 = nn.BatchNorm2d(self.total_conv_channels)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        self.var_pool = lambda x: torch.var(x, dim=1, keepdim=True)\n",
        "        self.avg_pool = lambda x: torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        self.norm_t = nn.LayerNorm(55)\n",
        "        self.norm_s = nn.LayerNorm(100)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        self.feedforward_1 = nn.Sequential(\n",
        "            nn.Linear(55, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 55)\n",
        "        )\n",
        "        self.feedforward_2 = nn.Sequential(\n",
        "            nn.Linear(100, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 100)\n",
        "        )\n",
        "\n",
        "        self.mamba_t = Mamba(d_model=55, d_state=8)\n",
        "        self.mamba_s = Mamba(d_model=100, d_state=8)\n",
        "\n",
        "        self.conv_t = ConvEncoder(in_channels=110, out_channels=55)\n",
        "        self.conv_s = ConvEncoder(in_channels=200, out_channels=100)\n",
        "\n",
        "        self.pool_t = nn.Identity()\n",
        "        self.pool_s = nn.Identity()\n",
        "\n",
        "        self.fc_s = nn.Linear(55*100, 32)\n",
        "        self.fc_t = nn.Linear(100 * 55, 32)\n",
        "\n",
        "        self.fc_class = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x, mode=\"classification\"):\n",
        "        x = x.unsqueeze(1)  # [B, 1, T, C]\n",
        "        x_convs = [conv(x) for conv in self.temp_convs]\n",
        "        x = torch.cat(x_convs, dim=1)  # [B, total_conv_channels, T, C]\n",
        "\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x_var = self.var_pool(x)  # [B, 1, T, C]\n",
        "        x_avg = self.avg_pool(x)\n",
        "\n",
        "        x_var = x_var.permute(0, 2, 3, 1).squeeze(-1)  # [B, T, C]\n",
        "        x_avg = x_avg.permute(0, 2, 3, 1).squeeze(-1)\n",
        "\n",
        "        def shared_mamba_block(x, mamba, norm, feedforward):\n",
        "            res1 = x\n",
        "            x = norm(x)\n",
        "            x = mamba(x)\n",
        "            x = self.dropout(x) + res1\n",
        "            res2 = x\n",
        "            x = norm(x)\n",
        "            x = feedforward(x) + res2\n",
        "            return x\n",
        "\n",
        "        x_tvar = shared_mamba_block(x_var.permute(0, 2, 1), self.mamba_t, self.norm_t, self.feedforward_1)\n",
        "        x_tavg = shared_mamba_block(x_avg.permute(0, 2, 1), self.mamba_t, self.norm_t, self.feedforward_1)\n",
        "        x_t = torch.cat([x_tvar, x_tavg], dim=-1)  # [B, 55, 100] × 2 → [B, 110, 100]\n",
        "\n",
        "        x_svar = shared_mamba_block(x_var, self.mamba_s, self.norm_s, self.feedforward_2)\n",
        "        x_savg = shared_mamba_block(x_avg, self.mamba_s, self.norm_s, self.feedforward_2)\n",
        "        x_s = torch.cat([x_svar, x_savg], dim=-1)  # [B, T, 200]\n",
        "\n",
        "        x_tconv = self.conv_t(x_t)  # [B, 100, 55]\n",
        "        x_sconv = self.conv_s(x_s)  #[B,55,100]\n",
        "        x_sconv = x_sconv.permute(0, 2, 1)\n",
        "\n",
        "        x_tconv = self.pool_t(x_tconv).reshape(x_tconv.shape[0], -1)  # [B, 100 * 55]\n",
        "        x_sconv = self.pool_s(x_sconv).reshape(x_sconv.shape[0], -1)  # [B, 10 * 55 = 550]\n",
        "\n",
        "\n",
        "        x_sfc = self.fc_s(x_sconv)\n",
        "        x_tfc = self.fc_t(x_tconv)\n",
        "\n",
        "        x_fused = torch.cat([x_sfc, x_tfc], dim=1)  # [B, 64]\n",
        "\n",
        "        if mode == \"classification\":\n",
        "            return self.fc_class(x_fused)\n"
      ],
      "metadata": {
        "id": "_j5640_Fq_ag"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Training Hyperparameters\n",
        "batch_size = 32\n",
        "num_epochs_train = 30  # Classification\n",
        "learning_rate = 0.0001\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "# Data Preparation\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)  # (N_train, 55, 100)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Model Init\n",
        "model = STMambaNet(input_size=100, hidden_size=128, num_classes=2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer, Loss, Scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
        "\n",
        "# Early Stopping Setup\n",
        "best_val_acc = 0\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "num_epochs = 30\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    running_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, mode=\"classification\")\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs, mode=\"classification\")\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Loss: {running_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")  # Save best model\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "# Load best model after training\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e288346-01e9-40f5-d104-799238900f4e",
        "id": "jgAl6_0GrGJf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Acc: 0.6236 | Val Acc: 0.6548 | Loss: 902.1159\n",
            "Epoch 2: Train Acc: 0.6557 | Val Acc: 0.6517 | Loss: 791.1144\n",
            "Epoch 3: Train Acc: 0.6610 | Val Acc: 0.6550 | Loss: 781.0582\n",
            "Epoch 4: Train Acc: 0.6654 | Val Acc: 0.6616 | Loss: 772.7375\n",
            "Epoch 5: Train Acc: 0.6671 | Val Acc: 0.6693 | Loss: 771.3655\n",
            "Epoch 6: Train Acc: 0.6705 | Val Acc: 0.6594 | Loss: 768.6213\n",
            "Epoch 7: Train Acc: 0.6725 | Val Acc: 0.6679 | Loss: 765.1288\n",
            "Epoch 8: Train Acc: 0.6731 | Val Acc: 0.6657 | Loss: 763.7765\n",
            "Epoch 9: Train Acc: 0.6738 | Val Acc: 0.6683 | Loss: 762.4169\n",
            "Epoch 10: Train Acc: 0.6745 | Val Acc: 0.6529 | Loss: 761.1518\n",
            "Epoch 11: Train Acc: 0.6746 | Val Acc: 0.6679 | Loss: 760.8084\n",
            "Epoch 12: Train Acc: 0.6750 | Val Acc: 0.6644 | Loss: 759.2342\n",
            "Epoch 13: Train Acc: 0.6748 | Val Acc: 0.6707 | Loss: 761.0997\n",
            "Epoch 14: Train Acc: 0.6760 | Val Acc: 0.6725 | Loss: 758.8101\n",
            "Epoch 15: Train Acc: 0.6755 | Val Acc: 0.6648 | Loss: 759.0008\n",
            "Epoch 16: Train Acc: 0.6736 | Val Acc: 0.6679 | Loss: 760.8042\n",
            "Epoch 17: Train Acc: 0.6763 | Val Acc: 0.6731 | Loss: 757.4461\n",
            "Epoch 18: Train Acc: 0.6758 | Val Acc: 0.6764 | Loss: 758.0875\n",
            "Epoch 19: Train Acc: 0.6775 | Val Acc: 0.6659 | Loss: 758.7748\n",
            "Epoch 20: Train Acc: 0.6760 | Val Acc: 0.6713 | Loss: 756.5343\n",
            "Epoch 21: Train Acc: 0.6722 | Val Acc: 0.6659 | Loss: 758.9759\n",
            "Epoch 22: Train Acc: 0.6740 | Val Acc: 0.6677 | Loss: 760.2437\n",
            "Epoch 23: Train Acc: 0.6774 | Val Acc: 0.6764 | Loss: 757.5946\n",
            "Epoch 24: Train Acc: 0.6772 | Val Acc: 0.6663 | Loss: 756.7373\n",
            "Epoch 25: Train Acc: 0.6782 | Val Acc: 0.6691 | Loss: 755.1241\n",
            "Epoch 26: Train Acc: 0.6771 | Val Acc: 0.6776 | Loss: 757.5372\n",
            "Epoch 27: Train Acc: 0.6760 | Val Acc: 0.6707 | Loss: 758.9633\n",
            "Epoch 28: Train Acc: 0.6804 | Val Acc: 0.6687 | Loss: 755.7312\n",
            "Epoch 29: Train Acc: 0.6770 | Val Acc: 0.6653 | Loss: 755.4826\n",
            "Epoch 30: Train Acc: 0.6784 | Val Acc: 0.6693 | Loss: 755.9492\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def evaluate_test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_test_batch, y_test_batch in test_loader:\n",
        "            X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "\n",
        "            outputs = model(X_test_batch, mode=\"classification\")\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (preds == y_test_batch).sum().item()\n",
        "            total += y_test_batch.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(y_test_batch.cpu().numpy())\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    cm_display = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    cm_display.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "evaluate_test(model, test_loader)\n"
      ],
      "metadata": {
        "outputId": "baf59c28-0f14-457b-fc6b-dc6081641e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "GhhDuXnW7bnP"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6840\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed version of the hyperparameter search with correct tensor shapes\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from mamba_ssm import Mamba\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class FixedSTMambaNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes, config):\n",
        "        super(FixedSTMambaNet, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Use config parameters\n",
        "        conv_channels = config.get('conv_channels', [4, 4])\n",
        "        dropout_rate = config.get('dropout_rate', 0.3)\n",
        "        mamba_d_state = config.get('mamba_d_state', 8)\n",
        "\n",
        "        self.kernel_sizes = [5, 9]\n",
        "        self.conv_channels = conv_channels\n",
        "\n",
        "        # Temporal convolutions\n",
        "        self.temp_convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=out_ch,\n",
        "                     kernel_size=(1, k), padding=(0, k // 2))\n",
        "            for k, out_ch in zip(self.kernel_sizes, self.conv_channels)\n",
        "        ])\n",
        "\n",
        "        self.total_conv_channels = sum(self.conv_channels)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(self.total_conv_channels)\n",
        "        self.spatial_conv = nn.Conv2d(in_channels=self.total_conv_channels,\n",
        "                                    out_channels=self.total_conv_channels,\n",
        "                                    kernel_size=(1, 1))\n",
        "        self.batch_norm2 = nn.BatchNorm2d(self.total_conv_channels)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "        # Pooling operations\n",
        "        self.var_pool = lambda x: torch.var(x, dim=1, keepdim=True)\n",
        "        self.avg_pool = lambda x: torch.mean(x, dim=1, keepdim=True)\n",
        "\n",
        "        # Layer norms\n",
        "        self.norm_t = nn.LayerNorm(55)\n",
        "        self.norm_s = nn.LayerNorm(100)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Feedforward networks\n",
        "        self.feedforward_1 = nn.Sequential(\n",
        "            nn.Linear(55, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 55)\n",
        "        )\n",
        "        self.feedforward_2 = nn.Sequential(\n",
        "            nn.Linear(100, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 100)\n",
        "        )\n",
        "\n",
        "        # Mamba layers with configurable d_state\n",
        "        self.mamba_t = Mamba(d_model=55, d_state=mamba_d_state)\n",
        "        self.mamba_s = Mamba(d_model=100, d_state=mamba_d_state)\n",
        "\n",
        "        # Add conv encoders to handle different channel sizes\n",
        "        self.conv_t = self._make_conv_encoder(110, 55)\n",
        "        self.conv_s = self._make_conv_encoder(200, 100)\n",
        "\n",
        "        # Identity pooling (keeping original behavior)\n",
        "        self.pool_t = nn.Identity()\n",
        "        self.pool_s = nn.Identity()\n",
        "\n",
        "        # FIXED: Properly calculate FC input sizes\n",
        "        # After processing, we get: temporal = (batch, 100, 55), spatial = (batch, 55, 100)\n",
        "        self.fc_s = nn.Linear(55 * 100, 32)  # spatial path: 55 channels × 100 time points\n",
        "        self.fc_t = nn.Linear(100 * 55, 32)  # temporal path: 100 time points × 55 channels\n",
        "\n",
        "        self.fc_class = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_conv_encoder(self, in_channels, out_channels):\n",
        "        \"\"\"Create a simple conv encoder to match dimensions\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mode=\"classification\"):\n",
        "        # Input: [batch, 55, 100]\n",
        "        x = x.unsqueeze(1)  # [batch, 1, 55, 100]\n",
        "\n",
        "        # Apply temporal convolutions\n",
        "        x_convs = [conv(x) for conv in self.temp_convs]  # List of [batch, conv_ch, 55, 100]\n",
        "        x = torch.cat(x_convs, dim=1)  # [batch, total_conv_channels, 55, 100]\n",
        "\n",
        "        # Batch norm and spatial conv\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.spatial_conv(x)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        # Pooling over conv channels\n",
        "        x_var = self.var_pool(x)  # [batch, 1, 55, 100]\n",
        "        x_avg = self.avg_pool(x)  # [batch, 1, 55, 100]\n",
        "\n",
        "        # Remove channel dimension and prepare for Mamba\n",
        "        x_var = x_var.squeeze(1)  # [batch, 55, 100]\n",
        "        x_avg = x_avg.squeeze(1)  # [batch, 55, 100]\n",
        "\n",
        "        def shared_mamba_block(x, mamba, norm, feedforward):\n",
        "            res1 = x\n",
        "            x = norm(x)\n",
        "            x = mamba(x)\n",
        "            x = self.dropout(x) + res1\n",
        "            res2 = x\n",
        "            x = norm(x)\n",
        "            x = feedforward(x) + res2\n",
        "            return x\n",
        "\n",
        "        # Temporal path: process along channel dimension (55)\n",
        "        x_tvar = shared_mamba_block(x_var.permute(0, 2, 1), self.mamba_t,\n",
        "                                  self.norm_t, self.feedforward_1)  # [batch, 100, 55]\n",
        "        x_tavg = shared_mamba_block(x_avg.permute(0, 2, 1), self.mamba_t,\n",
        "                                  self.norm_t, self.feedforward_1)  # [batch, 100, 55]\n",
        "        x_t = torch.cat([x_tvar, x_tavg], dim=-1)  # [batch, 100, 110]\n",
        "\n",
        "        # Spatial path: process along time dimension (100)\n",
        "        x_svar = shared_mamba_block(x_var, self.mamba_s, self.norm_s, self.feedforward_2)  # [batch, 55, 100]\n",
        "        x_savg = shared_mamba_block(x_avg, self.mamba_s, self.norm_s, self.feedforward_2)  # [batch, 55, 100]\n",
        "        x_s = torch.cat([x_svar, x_savg], dim=-1)  # [batch, 55, 200]\n",
        "\n",
        "        # Apply conv encoders to reduce dimensions\n",
        "        x_tconv = self.conv_t(x_t.permute(0, 2, 1)).permute(0, 2, 1)  # [batch, 100, 55]\n",
        "        x_sconv = self.conv_s(x_s.permute(0, 2, 1)).permute(0, 2, 1)  # [batch, 55, 100]\n",
        "\n",
        "        # Apply pooling and flatten\n",
        "        x_tconv = self.pool_t(x_tconv).reshape(x_tconv.shape[0], -1)  # [batch, 100*55]\n",
        "        x_sconv = self.pool_s(x_sconv).reshape(x_sconv.shape[0], -1)  # [batch, 55*100]\n",
        "\n",
        "        # Fully connected layers\n",
        "        x_sfc = self.fc_s(x_sconv)  # [batch, 32]\n",
        "        x_tfc = self.fc_t(x_tconv)  # [batch, 32]\n",
        "\n",
        "        # Fusion and classification\n",
        "        x_fused = torch.cat([x_sfc, x_tfc], dim=1)  # [batch, 64]\n",
        "\n",
        "        if mode == \"classification\":\n",
        "            return self.fc_class(x_fused)\n",
        "\n",
        "class FixedHyperparameterSearchRunner:\n",
        "    def __init__(self, X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "                 results_dir=\"hyperparameter_results\"):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.X_val = X_val\n",
        "        self.y_val = y_val\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Setup results directory\n",
        "        self.results_dir = Path(results_dir)\n",
        "        self.results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Results tracking\n",
        "        self.results = []\n",
        "        self.best_config = None\n",
        "        self.best_score = 0.0\n",
        "\n",
        "        # Device setup\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "    def create_optimizer(self, model, config):\n",
        "        \"\"\"Create optimizer based on config\"\"\"\n",
        "        lr = config.get('learning_rate', 1e-3)\n",
        "        weight_decay = config.get('weight_decay', 1e-4)\n",
        "        optimizer_type = config.get('optimizer_type', 'adam')\n",
        "\n",
        "        if optimizer_type == 'adam':\n",
        "            return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        elif optimizer_type == 'adamw':\n",
        "            return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        elif optimizer_type == 'sgd':\n",
        "            return optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
        "        else:\n",
        "            return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    def train_single_config(self, config, max_epochs=20):\n",
        "        \"\"\"Train model with a single configuration\"\"\"\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training with config: {config}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        batch_size = config.get('batch_size', 32)\n",
        "\n",
        "        train_dataset = TensorDataset(\n",
        "            torch.tensor(self.X_train, dtype=torch.float32),\n",
        "            torch.tensor(self.y_train, dtype=torch.long)\n",
        "        )\n",
        "        val_dataset = TensorDataset(\n",
        "            torch.tensor(self.X_val, dtype=torch.float32),\n",
        "            torch.tensor(self.y_val, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Create model with fixed architecture\n",
        "        model = FixedSTMambaNet(\n",
        "            input_size=100,\n",
        "            hidden_size=config.get('hidden_size', 128),\n",
        "            num_classes=2,\n",
        "            config=config\n",
        "        )\n",
        "        model.to(self.device)\n",
        "\n",
        "        # Create optimizer and scheduler\n",
        "        optimizer = self.create_optimizer(model, config)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, mode='max', factor=0.5, patience=3, verbose=False\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "        patience = 7\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "            running_loss = 0\n",
        "\n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs, mode=\"classification\")\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                train_correct += (preds == labels).sum().item()\n",
        "                train_total += labels.size(0)\n",
        "\n",
        "            train_acc = train_correct / train_total\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                    outputs = model(inputs, mode=\"classification\")\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    val_correct += (preds == labels).sum().item()\n",
        "                    val_total += labels.size(0)\n",
        "\n",
        "            val_acc = val_correct / val_total\n",
        "\n",
        "            print(f\"Epoch {epoch+1:2d}: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Loss: {running_loss:.4f}\")\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(val_acc)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(model.state_dict(), self.results_dir / \"temp_best_model.pt\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Test evaluation with best model\n",
        "        model.load_state_dict(torch.load(self.results_dir / \"temp_best_model.pt\"))\n",
        "        model.eval()\n",
        "\n",
        "        test_dataset = TensorDataset(\n",
        "            torch.tensor(self.X_test, dtype=torch.float32),\n",
        "            torch.tensor(self.y_test, dtype=torch.long)\n",
        "        )\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        test_correct = 0\n",
        "        test_total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = model(inputs, mode=\"classification\")\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                test_correct += (preds == labels).sum().item()\n",
        "                test_total += labels.size(0)\n",
        "\n",
        "        test_acc = test_correct / test_total\n",
        "\n",
        "        results = {\n",
        "            'val_accuracy': best_val_acc,\n",
        "            'test_accuracy': test_acc,\n",
        "            'final_train_accuracy': train_acc,\n",
        "            'epochs_trained': epoch + 1\n",
        "        }\n",
        "\n",
        "        print(f\"✅ Results: Val Acc: {best_val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
        "\n",
        "        # Clean up\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def run_search(self, search_configs, max_experiments=None, random_search=False):\n",
        "        \"\"\"Run hyperparameter search\"\"\"\n",
        "\n",
        "        import random as rnd\n",
        "        if random_search and max_experiments:\n",
        "            search_configs = rnd.sample(search_configs, min(max_experiments, len(search_configs)))\n",
        "        elif max_experiments:\n",
        "            search_configs = search_configs[:max_experiments]\n",
        "\n",
        "        print(f\"Running hyperparameter search with {len(search_configs)} configurations...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, config in enumerate(search_configs):\n",
        "            print(f\"\\n🔍 Experiment {i+1}/{len(search_configs)}\")\n",
        "\n",
        "            try:\n",
        "                # Train with this configuration\n",
        "                results = self.train_single_config(config)\n",
        "\n",
        "                # Store results\n",
        "                experiment_result = {\n",
        "                    'experiment_id': i,\n",
        "                    'config': config,\n",
        "                    'results': results,\n",
        "                    'timestamp': time.time()\n",
        "                }\n",
        "\n",
        "                self.results.append(experiment_result)\n",
        "\n",
        "                # Update best configuration\n",
        "                val_acc = results['val_accuracy']\n",
        "                if val_acc > self.best_score:\n",
        "                    self.best_score = val_acc\n",
        "                    self.best_config = config.copy()\n",
        "                    # Save best model permanently\n",
        "                    import shutil\n",
        "                    shutil.copy(\n",
        "                        self.results_dir / \"temp_best_model.pt\",\n",
        "                        self.results_dir / \"best_model.pt\"\n",
        "                    )\n",
        "\n",
        "                print(f\"✅ Completed experiment {i+1} | Val Acc: {val_acc:.4f} | Best so far: {self.best_score:.4f}\")\n",
        "\n",
        "                # Save intermediate results\n",
        "                self.save_results()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Experiment {i+1} failed: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n🏁 Search completed in {total_time/60:.1f} minutes\")\n",
        "        print(f\"🏆 Best validation accuracy: {self.best_score:.4f}\")\n",
        "        print(f\"🏆 Best configuration: {self.best_config}\")\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save results to files\"\"\"\n",
        "        if not self.results:\n",
        "            return\n",
        "\n",
        "        # Save detailed results\n",
        "        with open(self.results_dir / \"search_results.json\", 'w') as f:\n",
        "            json.dump(self.results, f, indent=2)\n",
        "\n",
        "        # Save best config\n",
        "        with open(self.results_dir / \"best_config.json\", 'w') as f:\n",
        "            json.dump({\n",
        "                'best_score': self.best_score,\n",
        "                'best_config': self.best_config\n",
        "            }, f, indent=2)\n",
        "\n",
        "        # Create summary DataFrame\n",
        "        summary_data = []\n",
        "        for result in self.results:\n",
        "            row = result['config'].copy()\n",
        "            row.update(result['results'])\n",
        "            row['experiment_id'] = result['experiment_id']\n",
        "            summary_data.append(row)\n",
        "\n",
        "        df = pd.DataFrame(summary_data)\n",
        "        df.to_csv(self.results_dir / \"search_summary.csv\", index=False)\n",
        "\n",
        "        print(f\"📁 Results saved to {self.results_dir}\")\n",
        "\n",
        "# Safer configuration set (smaller changes from original)\n",
        "def create_safe_search_configs():\n",
        "    \"\"\"Create safer configurations that are closer to the original\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            'learning_rate': 1e-3,      # 10x increase (conservative)\n",
        "            'mamba_d_state': 16,        # 2x increase\n",
        "            'dropout_rate': 0.2,        # Slight decrease\n",
        "            'batch_size': 64,           # 2x increase\n",
        "            'hidden_size': 128,         # Same\n",
        "            'conv_channels': [4, 4],    # Same as original\n",
        "            'weight_decay': 1e-4,       # Same\n",
        "        },\n",
        "        {\n",
        "            'learning_rate': 5e-4,      # 5x increase\n",
        "            'mamba_d_state': 16,        # 2x increase\n",
        "            'dropout_rate': 0.25,       # Slight decrease\n",
        "            'batch_size': 32,           # Same\n",
        "            'hidden_size': 128,         # Same\n",
        "            'conv_channels': [6, 6],    # Moderate increase\n",
        "            'weight_decay': 1e-4,       # Same\n",
        "        },\n",
        "        {\n",
        "            'learning_rate': 2e-3,      # 20x increase (aggressive)\n",
        "            'mamba_d_state': 8,         # Same\n",
        "            'dropout_rate': 0.1,        # Big decrease\n",
        "            'batch_size': 64,           # 2x increase\n",
        "            'hidden_size': 256,         # 2x increase\n",
        "            'conv_channels': [4, 4],    # Same\n",
        "            'weight_decay': 1e-3,       # 10x increase\n",
        "        }\n",
        "    ]\n",
        "\n",
        "print(\"✅ Fixed hyperparameter search runner ready!\")\n",
        "print(\"Run the cell below to start the search.\")"
      ],
      "metadata": {
        "id": "a5z9mRqfFg54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9430e3-adfb-4685-a84b-3eb4f86266bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed hyperparameter search runner ready!\n",
            "Run the cell below to start the search.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell after copying the fixed code above\n",
        "\n",
        "# Initialize the FIXED search runner\n",
        "runner = FixedHyperparameterSearchRunner(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_val=X_val,\n",
        "    y_val=y_val,\n",
        "    X_test=X_test,\n",
        "    y_test=y_test,\n",
        "    results_dir=\"eeg_hyperparameter_results_fixed\"\n",
        ")\n",
        "\n",
        "# Use safer configurations (smaller changes from original)\n",
        "print(\"🚀 Starting FIXED hyperparameter search with safer configurations...\")\n",
        "\n",
        "safe_configs = create_safe_search_configs()\n",
        "\n",
        "print(\"\\nConfigurations to test:\")\n",
        "for i, config in enumerate(safe_configs):\n",
        "    print(f\"Config {i+1}: LR={config['learning_rate']}, d_state={config['mamba_d_state']}, \"\n",
        "          f\"dropout={config['dropout_rate']}, batch={config['batch_size']}\")\n",
        "\n",
        "# Run the search\n",
        "results = runner.run_search(safe_configs)\n",
        "\n",
        "# Check results\n",
        "print(\"\\n📊 FINAL RESULTS:\")\n",
        "print(f\"Best validation accuracy: {runner.best_score:.4f}\")\n",
        "print(f\"Best configuration: {runner.best_config}\")\n",
        "\n",
        "# Display results if any succeeded\n",
        "if runner.results:\n",
        "    df = pd.read_csv(\"eeg_hyperparameter_results_fixed/search_summary.csv\")\n",
        "    print(\"\\n📈 All results:\")\n",
        "    print(df[['val_accuracy', 'test_accuracy', 'learning_rate', 'mamba_d_state', 'dropout_rate', 'batch_size']])\n",
        "else:\n",
        "    print(\"❌ No experiments succeeded. Check the error messages above.\")"
      ],
      "metadata": {
        "id": "QWi3sLUtFg20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c748250d-9a4c-45c8-bf85-33d916476135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "🚀 Starting FIXED hyperparameter search with safer configurations...\n",
            "\n",
            "Configurations to test:\n",
            "Config 1: LR=0.001, d_state=16, dropout=0.2, batch=64\n",
            "Config 2: LR=0.0005, d_state=16, dropout=0.25, batch=32\n",
            "Config 3: LR=0.002, d_state=8, dropout=0.1, batch=64\n",
            "Running hyperparameter search with 3 configurations...\n",
            "\n",
            "🔍 Experiment 1/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.001, 'mamba_d_state': 16, 'dropout_rate': 0.2, 'batch_size': 64, 'hidden_size': 128, 'conv_channels': [4, 4], 'weight_decay': 0.0001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5448 | Val Acc: 0.5903 | Loss: 326.6547\n",
            "Epoch  2: Train Acc: 0.5927 | Val Acc: 0.6040 | Loss: 306.8947\n",
            "Epoch  3: Train Acc: 0.6158 | Val Acc: 0.6199 | Loss: 299.2024\n",
            "Epoch  4: Train Acc: 0.6266 | Val Acc: 0.6172 | Loss: 294.2930\n",
            "Epoch  5: Train Acc: 0.6385 | Val Acc: 0.6133 | Loss: 289.4210\n",
            "Epoch  6: Train Acc: 0.6527 | Val Acc: 0.6106 | Loss: 283.2629\n",
            "Epoch  7: Train Acc: 0.6697 | Val Acc: 0.5873 | Loss: 275.7833\n",
            "Epoch  8: Train Acc: 0.7120 | Val Acc: 0.6100 | Loss: 255.6236\n",
            "Epoch  9: Train Acc: 0.7398 | Val Acc: 0.5963 | Loss: 237.9264\n",
            "Epoch 10: Train Acc: 0.7637 | Val Acc: 0.6073 | Loss: 221.9207\n",
            "Early stopping at epoch 10\n",
            "✅ Results: Val Acc: 0.6199 | Test Acc: 0.6114\n",
            "✅ Completed experiment 1 | Val Acc: 0.6199 | Best so far: 0.6199\n",
            "📁 Results saved to eeg_hyperparameter_results_fixed\n",
            "\n",
            "🔍 Experiment 2/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.0005, 'mamba_d_state': 16, 'dropout_rate': 0.25, 'batch_size': 32, 'hidden_size': 128, 'conv_channels': [6, 6], 'weight_decay': 0.0001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5665 | Val Acc: 0.5966 | Loss: 639.4688\n",
            "Epoch  2: Train Acc: 0.6134 | Val Acc: 0.6377 | Loss: 597.5911\n",
            "Epoch  3: Train Acc: 0.6319 | Val Acc: 0.6306 | Loss: 582.1992\n",
            "Epoch  4: Train Acc: 0.6464 | Val Acc: 0.6388 | Loss: 573.9705\n",
            "Epoch  5: Train Acc: 0.6534 | Val Acc: 0.6372 | Loss: 566.5780\n",
            "Epoch  6: Train Acc: 0.6643 | Val Acc: 0.6278 | Loss: 556.1444\n",
            "Epoch  7: Train Acc: 0.6796 | Val Acc: 0.6361 | Loss: 543.7978\n",
            "Epoch  8: Train Acc: 0.6979 | Val Acc: 0.6300 | Loss: 524.4862\n",
            "Epoch  9: Train Acc: 0.7307 | Val Acc: 0.6270 | Loss: 485.9522\n",
            "Epoch 10: Train Acc: 0.7538 | Val Acc: 0.6265 | Loss: 457.8411\n",
            "Epoch 11: Train Acc: 0.7726 | Val Acc: 0.6215 | Loss: 433.8111\n",
            "Early stopping at epoch 11\n",
            "✅ Results: Val Acc: 0.6388 | Test Acc: 0.6350\n",
            "✅ Completed experiment 2 | Val Acc: 0.6388 | Best so far: 0.6388\n",
            "📁 Results saved to eeg_hyperparameter_results_fixed\n",
            "\n",
            "🔍 Experiment 3/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.002, 'mamba_d_state': 8, 'dropout_rate': 0.1, 'batch_size': 64, 'hidden_size': 256, 'conv_channels': [4, 4], 'weight_decay': 0.001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5504 | Val Acc: 0.5887 | Loss: 325.6152\n",
            "Epoch  2: Train Acc: 0.5983 | Val Acc: 0.5898 | Loss: 303.6353\n",
            "Epoch  3: Train Acc: 0.6068 | Val Acc: 0.5919 | Loss: 299.0707\n",
            "Epoch  4: Train Acc: 0.6153 | Val Acc: 0.5996 | Loss: 297.0427\n",
            "Epoch  5: Train Acc: 0.6205 | Val Acc: 0.6062 | Loss: 295.0733\n",
            "Epoch  6: Train Acc: 0.6227 | Val Acc: 0.6095 | Loss: 293.9723\n",
            "Epoch  7: Train Acc: 0.6252 | Val Acc: 0.6095 | Loss: 293.0706\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3944764188>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Run the search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Check results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mrun_search\u001b[0;34m(self, search_configs, max_experiments, random_search)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Train with this configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mtrain_single_config\u001b[0;34m(self, config, max_epochs)\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import random\n",
        "\n",
        "def create_grid_search_configs():\n",
        "    \"\"\"Create full grid search configurations\"\"\"\n",
        "    configs = {\n",
        "        'learning_rate': [5e-4, 1e-3, 2e-3],\n",
        "        'mamba_d_state': [8, 16, 32],\n",
        "        'dropout_rate': [0.1, 0.2, 0.3],\n",
        "        'batch_size': [32, 64],\n",
        "        'hidden_size': [128, 256],\n",
        "        'conv_channels': [[4, 4], [8, 8]],\n",
        "        'weight_decay': [1e-4, 1e-3],\n",
        "    }\n",
        "\n",
        "    keys = list(configs.keys())\n",
        "    values = list(configs.values())\n",
        "\n",
        "    search_space = []\n",
        "    for combination in itertools.product(*values):\n",
        "        config_dict = dict(zip(keys, combination))\n",
        "        search_space.append(config_dict)\n",
        "\n",
        "    return search_space\n",
        "\n",
        "def create_reduced_grid_search():\n",
        "    \"\"\"Create a smaller, more focused grid search\"\"\"\n",
        "    configs = {\n",
        "        'learning_rate': [1e-3, 2e-3],           # Focus on higher LR\n",
        "        'mamba_d_state': [16, 32],               # Skip smallest\n",
        "        'dropout_rate': [0.1, 0.2],              # Skip highest\n",
        "        'batch_size': [64],                      # Fix at 64\n",
        "        'hidden_size': [128, 256],               # Keep both\n",
        "        'conv_channels': [[4, 4], [8, 8]],       # Keep both\n",
        "        'weight_decay': [1e-4],                  # Fix weight decay\n",
        "    }\n",
        "\n",
        "    keys = list(configs.keys())\n",
        "    values = list(configs.values())\n",
        "\n",
        "    search_space = []\n",
        "    for combination in itertools.product(*values):\n",
        "        config_dict = dict(zip(keys, combination))\n",
        "        search_space.append(config_dict)\n",
        "\n",
        "    return search_space\n",
        "\n",
        "# OPTION 1: Full grid search (432 configs - takes 15-30 hours)\n",
        "print(\"🔍 OPTION 1: Full Grid Search\")\n",
        "full_grid_configs = create_grid_search_configs()\n",
        "print(f\"Total configurations: {len(full_grid_configs)}\")\n",
        "print(f\"Estimated time: {len(full_grid_configs) * 3} minutes = {len(full_grid_configs) * 3 / 60:.1f} hours\")\n",
        "\n",
        "# OPTION 2: Reduced grid search (16 configs - takes 1-2 hours)\n",
        "print(\"\\n🎯 OPTION 2: Reduced Grid Search (Recommended)\")\n",
        "reduced_grid_configs = create_reduced_grid_search()\n",
        "print(f\"Total configurations: {len(reduced_grid_configs)}\")\n",
        "print(f\"Estimated time: {len(reduced_grid_configs) * 3} minutes = {len(reduced_grid_configs) * 3 / 60:.1f} hours\")\n",
        "\n",
        "# OPTION 3: Random sample from full grid (20 configs - takes 1 hour)\n",
        "print(\"\\n🎲 OPTION 3: Random Sample from Full Grid\")\n",
        "random_sample_size = 20\n",
        "random_configs = random.sample(full_grid_configs, random_sample_size)\n",
        "print(f\"Total configurations: {len(random_configs)}\")\n",
        "print(f\"Estimated time: {len(random_configs) * 3} minutes = {len(random_configs) * 3 / 60:.1f} hours\")\n",
        "\n",
        "# Show sample configurations\n",
        "print(\"\\n📋 Sample configurations from reduced grid:\")\n",
        "for i, config in enumerate(reduced_grid_configs[:3]):\n",
        "    print(f\"Config {i+1}: {config}\")"
      ],
      "metadata": {
        "id": "ieXGqSb1Fi5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cae36ca-6550-404d-88b9-55bee11a08e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 OPTION 1: Full Grid Search\n",
            "Total configurations: 432\n",
            "Estimated time: 1296 minutes = 21.6 hours\n",
            "\n",
            "🎯 OPTION 2: Reduced Grid Search (Recommended)\n",
            "Total configurations: 32\n",
            "Estimated time: 96 minutes = 1.6 hours\n",
            "\n",
            "🎲 OPTION 3: Random Sample from Full Grid\n",
            "Total configurations: 20\n",
            "Estimated time: 60 minutes = 1.0 hours\n",
            "\n",
            "📋 Sample configurations from reduced grid:\n",
            "Config 1: {'learning_rate': 0.001, 'mamba_d_state': 16, 'dropout_rate': 0.1, 'batch_size': 64, 'hidden_size': 128, 'conv_channels': [4, 4], 'weight_decay': 0.0001}\n",
            "Config 2: {'learning_rate': 0.001, 'mamba_d_state': 16, 'dropout_rate': 0.1, 'batch_size': 64, 'hidden_size': 128, 'conv_channels': [8, 8], 'weight_decay': 0.0001}\n",
            "Config 3: {'learning_rate': 0.001, 'mamba_d_state': 16, 'dropout_rate': 0.1, 'batch_size': 64, 'hidden_size': 256, 'conv_channels': [4, 4], 'weight_decay': 0.0001}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOOSE ONE OF THESE OPTIONS TO RUN:\n",
        "\n",
        "# Make sure you have the FixedHyperparameterSearchRunner from the previous cell\n",
        "\n",
        "# ============================================================================\n",
        "# OPTION 1: QUICK REDUCED GRID SEARCH (Recommended to start)\n",
        "# Time: 1-2 hours, 16 experiments\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🎯 Running REDUCED GRID SEARCH (16 configs)\")\n",
        "configs = create_reduced_grid_search()\n",
        "\n",
        "runner = FixedHyperparameterSearchRunner(\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "    results_dir=\"grid_search_reduced\"\n",
        ")\n",
        "\n",
        "results = runner.run_search(configs)\n",
        "print(f\"\\n🏆 Best from reduced grid: {runner.best_score:.4f}\")"
      ],
      "metadata": {
        "id": "X2jm-at2N1ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================================================\n",
        "# OPTION 2: RANDOM SAMPLE FROM FULL GRID\n",
        "# Time: 1 hour, 20 experiments\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🎲 Running RANDOM SAMPLE from full grid (20 configs)\")\n",
        "full_configs = create_grid_search_configs()\n",
        "\n",
        "runner = FixedHyperparameterSearchRunner(\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "    results_dir=\"grid_search_random\"\n",
        ")\n",
        "\n",
        "results = runner.run_search(full_configs, max_experiments=20, random_search=True)\n",
        "print(f\"\\n🏆 Best from random sample: {runner.best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "HgtODvJ2miU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTION 3: FULL GRID SEARCH (Run overnight!)\n",
        "# Time: 15-30 hours, 432 experiments\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🔍 Running FULL GRID SEARCH (432 configs) - This will take 15-30 hours!\")\n",
        "full_configs = create_grid_search_configs()\n",
        "\n",
        "runner = FixedHyperparameterSearchRunner(\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test,\n",
        "    results_dir=\"grid_search_full\"\n",
        ")\n",
        "\n",
        "results = runner.run_search(full_configs)\n",
        "print(f\"\\n🏆 Best from full grid: {runner.best_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "5LNlVlk0mlze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# OPTION 4: SMART PROGRESSIVE SEARCH\n",
        "# Start small, then expand based on results\n",
        "# ============================================================================\n",
        "\n",
        "print(\"🧠 Running SMART PROGRESSIVE SEARCH\")\n",
        "\n",
        "# Step 1: Test a few promising configs\n",
        "safe_configs = create_safe_search_configs()\n",
        "runner.run_search(safe_configs)\n",
        "best_safe = runner.best_score\n",
        "\n",
        "# Step 2: If improvement, run reduced grid\n",
        "if best_safe > 0.63:\n",
        "    print(f\"✅ Safe configs achieved {best_safe:.3f}, running reduced grid...\")\n",
        "    reduced_configs = create_reduced_grid_search()\n",
        "    runner.run_search(reduced_configs)\n",
        "    best_reduced = runner.best_score\n",
        "\n",
        "    # Step 3: If still improving, run random sample\n",
        "    if best_reduced > 0.65:\n",
        "        print(f\"✅ Reduced grid achieved {best_reduced:.3f}, running random sample...\")\n",
        "        full_configs = create_grid_search_configs()\n",
        "        runner.run_search(full_configs, max_experiments=50, random_search=True)\n",
        "\n",
        "print(\"\\n📊 FINAL RESULTS:\")\n",
        "print(f\"Best validation accuracy: {runner.best_score:.4f}\")\n",
        "print(f\"Best test accuracy: {[r['results']['test_accuracy'] for r in runner.results if r['results']['val_accuracy'] == runner.best_score][0]:.4f}\")\n",
        "print(f\"Best configuration: {runner.best_config}\")\n",
        "\n",
        "# Analyze results\n",
        "if runner.results:\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(f\"{runner.results_dir}/search_summary.csv\")\n",
        "\n",
        "    print(f\"\\n📈 TOP 5 CONFIGURATIONS:\")\n",
        "    top_5 = df.nlargest(5, 'val_accuracy')\n",
        "    print(top_5[['val_accuracy', 'test_accuracy', 'learning_rate', 'mamba_d_state', 'dropout_rate', 'batch_size']])\n",
        "\n",
        "    print(f\"\\n📊 ANALYSIS:\")\n",
        "    print(f\"Learning rate - Best performers: {top_5['learning_rate'].value_counts().head(3).to_dict()}\")\n",
        "    print(f\"Mamba d_state - Best performers: {top_5['mamba_d_state'].value_counts().head(3).to_dict()}\")\n",
        "    print(f\"Dropout rate - Best performers: {top_5['dropout_rate'].value_counts().head(3).to_dict()}\")\n",
        "\n",
        "    # Find improvement over baseline\n",
        "    baseline_acc = 0.6443  # Your original accuracy\n",
        "    best_acc = runner.best_score\n",
        "    improvement = (best_acc - baseline_acc) / baseline_acc * 100\n",
        "    print(f\"\\n🚀 IMPROVEMENT: {improvement:+.1f}% over baseline ({baseline_acc:.4f} → {best_acc:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k5AZkvIZmqDX",
        "outputId": "938e1469-0618-49f0-e5af-e1ed2fdc17c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Running SMART PROGRESSIVE SEARCH\n",
            "Running hyperparameter search with 3 configurations...\n",
            "\n",
            "🔍 Experiment 1/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.001, 'mamba_d_state': 16, 'dropout_rate': 0.2, 'batch_size': 64, 'hidden_size': 128, 'conv_channels': [4, 4], 'weight_decay': 0.0001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5422 | Val Acc: 0.5870 | Loss: 327.1534\n",
            "Epoch  2: Train Acc: 0.5993 | Val Acc: 0.6122 | Loss: 304.2261\n",
            "Epoch  3: Train Acc: 0.6209 | Val Acc: 0.6243 | Loss: 296.4023\n",
            "Epoch  4: Train Acc: 0.6359 | Val Acc: 0.6243 | Loss: 290.0640\n",
            "Epoch  5: Train Acc: 0.6507 | Val Acc: 0.5969 | Loss: 284.0278\n",
            "Epoch  6: Train Acc: 0.6680 | Val Acc: 0.6235 | Loss: 277.0415\n",
            "Epoch  7: Train Acc: 0.6884 | Val Acc: 0.6287 | Loss: 267.9443\n",
            "Epoch  8: Train Acc: 0.7089 | Val Acc: 0.6062 | Loss: 255.5249\n",
            "Epoch  9: Train Acc: 0.7406 | Val Acc: 0.6070 | Loss: 239.0148\n",
            "Epoch 10: Train Acc: 0.7624 | Val Acc: 0.6010 | Loss: 221.0621\n",
            "Epoch 11: Train Acc: 0.8011 | Val Acc: 0.5908 | Loss: 197.3134\n",
            "Epoch 12: Train Acc: 0.8531 | Val Acc: 0.5845 | Loss: 155.9926\n",
            "Epoch 13: Train Acc: 0.8777 | Val Acc: 0.5873 | Loss: 132.4357\n",
            "Epoch 14: Train Acc: 0.8977 | Val Acc: 0.5791 | Loss: 115.7880\n",
            "Early stopping at epoch 14\n",
            "✅ Results: Val Acc: 0.6287 | Test Acc: 0.6073\n",
            "✅ Completed experiment 1 | Val Acc: 0.6287 | Best so far: 0.6388\n",
            "📁 Results saved to eeg_hyperparameter_results_fixed\n",
            "\n",
            "🔍 Experiment 2/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.0005, 'mamba_d_state': 16, 'dropout_rate': 0.25, 'batch_size': 32, 'hidden_size': 128, 'conv_channels': [6, 6], 'weight_decay': 0.0001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5609 | Val Acc: 0.6010 | Loss: 640.5316\n",
            "Epoch  2: Train Acc: 0.6120 | Val Acc: 0.6169 | Loss: 599.5400\n",
            "Epoch  3: Train Acc: 0.6306 | Val Acc: 0.6257 | Loss: 585.8876\n",
            "Epoch  4: Train Acc: 0.6394 | Val Acc: 0.6399 | Loss: 576.4597\n",
            "Epoch  5: Train Acc: 0.6511 | Val Acc: 0.6437 | Loss: 567.8806\n",
            "Epoch  6: Train Acc: 0.6621 | Val Acc: 0.6366 | Loss: 558.6915\n",
            "Epoch  7: Train Acc: 0.6735 | Val Acc: 0.6336 | Loss: 545.8092\n",
            "Epoch  8: Train Acc: 0.6939 | Val Acc: 0.6344 | Loss: 528.2389\n",
            "Epoch  9: Train Acc: 0.7151 | Val Acc: 0.6262 | Loss: 505.7606\n",
            "Epoch 10: Train Acc: 0.7546 | Val Acc: 0.6188 | Loss: 456.8910\n",
            "Epoch 11: Train Acc: 0.7771 | Val Acc: 0.6213 | Loss: 424.1813\n",
            "Epoch 12: Train Acc: 0.8016 | Val Acc: 0.6141 | Loss: 395.0339\n",
            "Early stopping at epoch 12\n",
            "✅ Results: Val Acc: 0.6437 | Test Acc: 0.6298\n",
            "✅ Completed experiment 2 | Val Acc: 0.6437 | Best so far: 0.6437\n",
            "📁 Results saved to eeg_hyperparameter_results_fixed\n",
            "\n",
            "🔍 Experiment 3/3\n",
            "\n",
            "==================================================\n",
            "Training with config: {'learning_rate': 0.002, 'mamba_d_state': 8, 'dropout_rate': 0.1, 'batch_size': 64, 'hidden_size': 256, 'conv_channels': [4, 4], 'weight_decay': 0.001}\n",
            "==================================================\n",
            "Epoch  1: Train Acc: 0.5339 | Val Acc: 0.5670 | Loss: 345.9713\n",
            "Epoch  2: Train Acc: 0.5872 | Val Acc: 0.5980 | Loss: 307.1371\n",
            "Epoch  3: Train Acc: 0.5999 | Val Acc: 0.5969 | Loss: 301.0048\n",
            "Epoch  4: Train Acc: 0.6112 | Val Acc: 0.5993 | Loss: 297.8914\n",
            "Epoch  5: Train Acc: 0.6183 | Val Acc: 0.5977 | Loss: 295.6584\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1319447725>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Step 1: Test a few promising configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msafe_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_safe_search_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_configs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mbest_safe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mrun_search\u001b[0;34m(self, search_configs, max_experiments, random_search)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Train with this configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_single_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mtrain_single_config\u001b[0;34m(self, config, max_epochs)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Spatial path: process along time dimension (100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mx_svar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_mamba_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward_2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 55, 100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mx_savg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_mamba_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward_2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 55, 100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mx_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_svar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_savg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, 55, 200]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1141126697>\u001b[0m in \u001b[0;36mshared_mamba_block\u001b[0;34m(x, mamba, norm, feedforward)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mres1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmamba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mres2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mamba_ssm/modules/mamba_simple.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, inference_params)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mconv_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_conv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update state (B D W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcausal_conv1d_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseqlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"silu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"swish\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}