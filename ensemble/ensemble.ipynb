{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13766,
     "status": "ok",
     "timestamp": 1753181426282,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "SdFwNJzmnu96",
    "outputId": "eda5c3e7-9554-4256-af4c-d468ad150e8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ninja\n",
      "  Downloading ninja-1.13.0-py3-none-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting cmake\n",
      "  Downloading cmake-4.1.0-py3-none-win_amd64.whl.metadata (6.5 kB)\n",
      "Downloading ninja-1.13.0-py3-none-win_amd64.whl (309 kB)\n",
      "Downloading cmake-4.1.0-py3-none-win_amd64.whl (37.6 MB)\n",
      "   ---------------------------------------- 0.0/37.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/37.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/37.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/37.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/37.6 MB 837.5 kB/s eta 0:00:45\n",
      "    --------------------------------------- 0.8/37.6 MB 931.2 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.0/37.6 MB 914.5 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.3/37.6 MB 944.7 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 1.3/37.6 MB 944.7 kB/s eta 0:00:39\n",
      "   - -------------------------------------- 1.6/37.6 MB 864.6 kB/s eta 0:00:42\n",
      "   - -------------------------------------- 1.8/37.6 MB 915.0 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.8/37.6 MB 915.0 kB/s eta 0:00:40\n",
      "   -- ------------------------------------- 2.1/37.6 MB 939.6 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 2.4/37.6 MB 938.3 kB/s eta 0:00:38\n",
      "   -- ------------------------------------- 2.6/37.6 MB 967.7 kB/s eta 0:00:37\n",
      "   --- ------------------------------------ 2.9/37.6 MB 1.0 MB/s eta 0:00:35\n",
      "   --- ------------------------------------ 3.1/37.6 MB 1.0 MB/s eta 0:00:34\n",
      "   --- ------------------------------------ 3.7/37.6 MB 1.1 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 3.9/37.6 MB 1.1 MB/s eta 0:00:31\n",
      "   ---- ----------------------------------- 4.5/37.6 MB 1.2 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 5.0/37.6 MB 1.3 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 5.2/37.6 MB 1.3 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 5.8/37.6 MB 1.3 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 6.3/37.6 MB 1.4 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 6.3/37.6 MB 1.4 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 7.1/37.6 MB 1.4 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 7.3/37.6 MB 1.5 MB/s eta 0:00:21\n",
      "   -------- ------------------------------- 7.9/37.6 MB 1.5 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 8.4/37.6 MB 1.5 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 9.2/37.6 MB 1.6 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 9.7/37.6 MB 1.6 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 10.2/37.6 MB 1.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 10.7/37.6 MB 1.7 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 11.8/37.6 MB 1.8 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 12.6/37.6 MB 1.9 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 13.6/37.6 MB 2.0 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 14.7/37.6 MB 2.1 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 15.5/37.6 MB 2.1 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 16.5/37.6 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 17.0/37.6 MB 2.2 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 18.9/37.6 MB 2.4 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 19.9/37.6 MB 2.4 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 21.2/37.6 MB 2.5 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 22.3/37.6 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 24.1/37.6 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 25.4/37.6 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 27.0/37.6 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 28.3/37.6 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 29.9/37.6 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 29.9/37.6 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 29.9/37.6 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 30.7/37.6 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 32.5/37.6 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 33.6/37.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 34.3/37.6 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 35.4/37.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 35.7/37.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  36.7/37.6 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 37.6/37.6 MB 3.2 MB/s  0:00:11\n",
      "Installing collected packages: ninja, cmake\n",
      "\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   -------------------- ------------------- 1/2 [cmake]\n",
      "   ---------------------------------------- 2/2 [cmake]\n",
      "\n",
      "Successfully installed cmake-4.1.0 ninja-1.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (0.23.0+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torchvision) (2.3.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\desktop\\ra\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting mamba-ssm\n",
      "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [21 lines of output]\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.8.0+cu128\n",
      "      \n",
      "      \n",
      "      <string>:118: UserWarning: mamba_ssm was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\USER\\Desktop\\RA\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\USER\\Desktop\\RA\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\USER\\Desktop\\RA\\venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 175, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\USER\\Desktop\\RA\\venv\\Lib\\site-packages\\setuptools\\build_meta.py\", line 377, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"c:\\Users\\USER\\Desktop\\RA\\venv\\Lib\\site-packages\\setuptools\\build_meta.py\", line 335, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 188, in <module>\n",
      "      NameError: name 'bare_metal_version' is not defined\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install ninja cmake\n",
    "%pip install torch torchvision torchaudio\n",
    "%pip install --no-build-isolation mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1753181444432,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "6NqRoyj-kVCf"
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "     \"MODEL_PATHS\": {\n",
    "         \"conformer\": \"C:\\Users\\USER\\Desktop\\RA\\ensemble\\best_conformer_default.ptt\",\n",
    "         \"stmambanet\": \"C:\\Users\\USER\\Desktop\\RA\\ensemble\\best_model_mamba.pt\",\n",
    "         \"conformer_opt\": \"C:\\Users\\USER\\Desktop\\RA\\ensemble\\best_conformer_with_freq.pt\",\n",
    "         \"eegnet\": None  # Path to saved EEGNet model (TensorFlow)\n",
    "     },\n",
    "     \"DATA_PATH_CH\": \"C:\\Users\\USER\\Desktop\\RA\\data_ch\",\n",
    "     \"DATA_PATH\": \"C:\\Users\\USER\\Desktop\\RA\\data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1753181868200,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "ef02c8d6",
    "outputId": "c58d55cd-1283-433b-e9bc-9e20d68eff10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated sys.path: ['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/', '/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Add the directory containing custom modules to the Python path\n",
    "sys.path.append('/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/')\n",
    "\n",
    "# Verify that the path was added\n",
    "print(f\"Updated sys.path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "error",
     "timestamp": 1753181869888,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "mdNa83Dcpetw",
    "outputId": "dc3e673c-4444-436a-a341-534cf2a61b31"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'conformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-23-229685541.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Import model architectures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconformer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconformer_opt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConformerWithFreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'conformer'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "# PyTorch specific imports for data handling and training\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import model architectures\n",
    "from conformer import Conformer\n",
    "from mamba_ssm import Mamba\n",
    "from conformer_opt import ConformerWithFreq\n",
    "\n",
    "# Data processing imports\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1753181705078,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "ALFPGWOyk6kB"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.norm = nn.BatchNorm1d(out_channels)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [B, T, C] → [B, C, T]\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = x.transpose(1, 2)  # [B, C, T] → [B, T, C]\n",
    "        return x\n",
    "\n",
    "class STMambaNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(STMambaNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.kernel_sizes = [5, 9]\n",
    "        self.conv_channels = [4, 4]\n",
    "\n",
    "        self.temp_convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=out_ch, kernel_size=(1, k), padding=(0, k // 2))\n",
    "            for k, out_ch in zip(self.kernel_sizes, self.conv_channels)\n",
    "        ])\n",
    "\n",
    "        self.total_conv_channels = sum(self.conv_channels)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(self.total_conv_channels)\n",
    "        self.spatial_conv = nn.Conv2d(in_channels=self.total_conv_channels, out_channels=self.total_conv_channels, kernel_size=(1, 1))\n",
    "        self.batch_norm2 = nn.BatchNorm2d(self.total_conv_channels)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "        self.var_pool = lambda x: torch.var(x, dim=1, keepdim=True)\n",
    "        self.avg_pool = lambda x: torch.mean(x, dim=1, keepdim=True)\n",
    "\n",
    "        # Размерность нормализации соответствует сохраненным весам\n",
    "        self.norm_t = nn.LayerNorm(55)  # В сохраненных весах это 55\n",
    "        self.norm_s = nn.LayerNorm(100) # В сохраненных весах это 100\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.feedforward_1 = nn.Sequential(\n",
    "            nn.Linear(55, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 55)\n",
    "        )\n",
    "        self.feedforward_2 = nn.Sequential(\n",
    "            nn.Linear(100, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 100)\n",
    "        )\n",
    "\n",
    "        self.mamba_t = Mamba(d_model=55, d_state=8)\n",
    "        self.mamba_s = Mamba(d_model=100, d_state=8)\n",
    "\n",
    "        self.conv_t = ConvEncoder(in_channels=110, out_channels=55)\n",
    "        self.conv_s = ConvEncoder(in_channels=200, out_channels=100)\n",
    "\n",
    "        self.pool_t = nn.Identity()\n",
    "        self.pool_s = nn.Identity()\n",
    "\n",
    "        self.fc_s = nn.Linear(55*100, 32)\n",
    "        self.fc_t = nn.Linear(100 * 55, 32)\n",
    "\n",
    "        self.fc_class = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x, mode=\"classification\"):\n",
    "        x = x.unsqueeze(1)\n",
    "        x_convs = [conv(x) for conv in self.temp_convs]\n",
    "        x = torch.cat(x_convs, dim=1)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.spatial_conv(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.activation(x)\n",
    "        x_var = self.var_pool(x)\n",
    "        x_avg = self.avg_pool(x)\n",
    "        x_var = x_var.permute(0, 2, 3, 1).squeeze(-1)\n",
    "        x_avg = x_avg.permute(0, 2, 3, 1).squeeze(-1)\n",
    "\n",
    "        def shared_mamba_block(x, mamba, norm, feedforward):\n",
    "            res1 = x\n",
    "            x = norm(x)\n",
    "            x = mamba(x)\n",
    "            x = self.dropout(x) + res1\n",
    "            res2 = x\n",
    "            x = norm(x)\n",
    "            x = feedforward(x) + res2\n",
    "            return x\n",
    "\n",
    "        x_tvar = shared_mamba_block(x_var.permute(0, 2, 1), self.mamba_t, self.norm_t, self.feedforward_1)\n",
    "        x_tavg = shared_mamba_block(x_avg.permute(0, 2, 1), self.mamba_t, self.norm_t, self.feedforward_1)\n",
    "        x_t = torch.cat([x_tvar, x_tavg], dim=-1)\n",
    "        x_svar = shared_mamba_block(x_var, self.mamba_s, self.norm_s, self.feedforward_2)\n",
    "        x_savg = shared_mamba_block(x_avg, self.mamba_s, self.norm_s, self.feedforward_2)\n",
    "        x_s = torch.cat([x_svar, x_savg], dim=-1)\n",
    "        x_tconv = self.conv_t(x_t)\n",
    "        x_sconv = self.conv_s(x_s)\n",
    "        x_sconv = x_sconv.permute(0, 2, 1)\n",
    "        x_tconv = self.pool_t(x_tconv).reshape(x_tconv.shape[0], -1)\n",
    "        x_sconv = self.pool_s(x_sconv).reshape(x_sconv.shape[0], -1)\n",
    "        x_sfc = self.fc_s(x_sconv)\n",
    "        x_tfc = self.fc_t(x_tconv)\n",
    "        x_fused = torch.cat([x_sfc, x_tfc], dim=1)\n",
    "\n",
    "        if mode == \"classification\":\n",
    "            return self.fc_class(x_fused)\n",
    "\n",
    "# --- FIX: Simplified EEGNetv2 without subject ID handling ---\n",
    "class EEGNetv2(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch implementation of EEGNet. This version is generalized and does not use subject IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_classes=2, Chans=55, Samples=100, dropoutRate=0.5,\n",
    "                 F1=16, D=2, F2=16, kernel_length=10):\n",
    "        super(EEGNetv2, self).__init__()\n",
    "        self.Chans = Chans\n",
    "        self.Samples = Samples\n",
    "\n",
    "        # Block 1: Temporal and Depthwise Convolutions\n",
    "        self.conv1 = nn.Conv2d(1, F1, (1, kernel_length), padding=(0, kernel_length // 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(F1)\n",
    "        self.depthwise_conv = nn.Conv2d(F1, F1 * D, (Chans, 1), groups=F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(F1 * D)\n",
    "        self.activation_elu = nn.ELU()\n",
    "        self.avg_pool1 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout1 = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # Block 2: Separable Convolution\n",
    "        self.separable_conv = nn.Sequential(\n",
    "            nn.Conv2d(F1 * D, F1 * D, (1, kernel_length), groups=F1 * D, padding=(0, kernel_length // 2), bias=False),\n",
    "            nn.Conv2d(F1 * D, F2, 1, bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(F2)\n",
    "        self.avg_pool2 = nn.AvgPool2d((1, 4))\n",
    "        self.dropout2 = nn.Dropout(dropoutRate)\n",
    "\n",
    "        # Classifier\n",
    "        flattened_size = F2 * (Samples // 4 // 4)\n",
    "        self.fc = nn.Linear(flattened_size, nb_classes)\n",
    "\n",
    "    def apply_constraints(self):\n",
    "        w = self.depthwise_conv.weight.data\n",
    "        norm = w.norm(2, dim=(0, 2, 3), keepdim=True)\n",
    "        desired = torch.clamp(norm, 0, 1.0)\n",
    "        w.copy_((w * desired / (1e-8 + norm)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation_elu(x)\n",
    "        x = self.avg_pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.separable_conv(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.activation_elu(x)\n",
    "        x = self.avg_pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x_flat = torch.flatten(x, 1)\n",
    "        output = self.fc(x_flat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1753179036760,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "Qc6-76Ydo2BB"
   },
   "outputs": [],
   "source": [
    "class EnsembleModel:\n",
    "    \"\"\"\n",
    "    Ансамблевая модель для классификации памяти с использованием мажоритарного голосования.\n",
    "    Все модели реализованы на PyTorch для избежания конфликтов фреймворков.\n",
    "    \"\"\"\n",
    "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.models = {}\n",
    "        self.model_paths = CONFIG[\"MODEL_PATHS\"]\n",
    "\n",
    "        # Параметры канальной выборки для разных датасетов\n",
    "        self.ch_ch = list(range(63))\n",
    "        self.g_ch = list(range(61))\n",
    "        self.ch_rem = [0, 2, 6, 8, 29, 31, 51, 55]\n",
    "        self.g_rem = [46, 49, 56, 58, 59, 60]\n",
    "\n",
    "        for i in self.ch_rem:\n",
    "            self.ch_ch.remove(i)\n",
    "        for j in self.g_rem:\n",
    "            self.g_ch.remove(j)\n",
    "\n",
    "    def load_models(self):\n",
    "        \"\"\"Загрузка моделей из сохраненных весов.\"\"\"\n",
    "        # Загрузка EEG-Conformer\n",
    "        try:\n",
    "            self.models[\"conformer\"] = Conformer(emb_size=40, depth=6, n_classes=2)\n",
    "            state_dict = torch.load(self.model_paths[\"conformer\"], map_location=self.device, weights_only=True)\n",
    "            self.models[\"conformer\"].load_state_dict(state_dict, strict=False)\n",
    "            self.models[\"conformer\"].to(self.device)\n",
    "            self.models[\"conformer\"].eval()\n",
    "            print(\"EEG-Conformer загружен успешно!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке EEG-Conformer: {e}\")\n",
    "            self.models[\"conformer\"] = None\n",
    "\n",
    "        # Загрузка STMambaNet\n",
    "        try:\n",
    "            self.models[\"stmambanet\"] = STMambaNet(input_size=100, hidden_size=128, num_classes=2)\n",
    "            self.models[\"stmambanet\"].load_state_dict(torch.load(self.model_paths[\"stmambanet\"], map_location=self.device, weights_only=True))\n",
    "            self.models[\"stmambanet\"].to(self.device)\n",
    "            self.models[\"stmambanet\"].eval()\n",
    "            print(\"STMambaNet загружен успешно!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке STMambaNet: {e}\")\n",
    "            self.models[\"stmambanet\"] = None\n",
    "\n",
    "        # --- FIX: Load the simplified EEGNetv2 without subject ID parameters ---\n",
    "        try:\n",
    "            self.models[\"eegnet\"] = EEGNetv2(nb_classes=2, Chans=55, Samples=100)\n",
    "            if self.model_paths[\"eegnet\"]:\n",
    "                self.models[\"eegnet\"].load_state_dict(torch.load(self.model_paths[\"eegnet\"], map_location=self.device))\n",
    "                print(\"EEGNet (PyTorch) загружен из файла успешно!\")\n",
    "            else:\n",
    "                print(\"Создана новая модель EEGNet (PyTorch, не обучена)\")\n",
    "            self.models[\"eegnet\"].to(self.device)\n",
    "            self.models[\"eegnet\"].eval()\n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при загрузке/создании EEGNet: {e}\")\n",
    "            self.models[\"eegnet\"] = None\n",
    "\n",
    "    # --- FIX: Updated data loading to perform global undersampling and remove subject IDs ---\n",
    "    def load_and_preprocess_data(self, data_path_ch, data_path, perform_balancing=False):\n",
    "        \"\"\"Загрузка и предобработка данных. Балансировка выполняется на всем наборе данных.\"\"\"\n",
    "        X_all, y_all = [], []\n",
    "        for subj in range(1, 10):\n",
    "            try:\n",
    "                mat_data = loadmat(f\"{data_path_ch}/ASK/sbj_{subj}.mat\")\n",
    "                X = np.array(mat_data['x_post'])[:-1, self.ch_ch, :100]\n",
    "                y = np.array(mat_data['label_next_ind'])[0, :X.shape[0]]  # Обрезаем метки до размера данных\n",
    "                print(f\"Загружен субъект {subj} (CH): X={X.shape}, y={y.shape}\")\n",
    "                X_all.append(X); y_all.append(y)\n",
    "            except Exception as e: print(f\"Ошибка при загрузке файла sbj_{subj}.mat (CH): {e}\")\n",
    "        for subj in range(1, 15):\n",
    "            try:\n",
    "                with h5py.File(f\"{data_path}/ASK/sbj_{subj}.mat\", 'r') as f:\n",
    "                    X = np.array(f['x_post'])[:, self.g_ch, :100]\n",
    "                    y = np.array(f['label_next_ind'])[:X.shape[0], 0]  # Обрезаем метки до размера данных\n",
    "                    print(f\"Загружен субъект {subj} (KR): X={X.shape}, y={y.shape}\")\n",
    "                    X_all.append(X); y_all.append(y)\n",
    "            except Exception as e: print(f\"Ошибка при загрузке файла sbj_{subj}.mat (KR): {e}\")\n",
    "\n",
    "        data = np.concatenate(X_all, axis=0)\n",
    "        labels = np.concatenate(y_all, axis=0)\n",
    "\n",
    "        # Проверяем совпадение размеров X и y\n",
    "        if data.shape[0] != labels.shape[0]:\n",
    "            print(f\"ВНИМАНИЕ: Несоответствие размеров данных ({data.shape[0]}) и меток ({labels.shape[0]})\")\n",
    "            min_samples = min(data.shape[0], labels.shape[0])\n",
    "            data = data[:min_samples]\n",
    "            labels = labels[:min_samples]\n",
    "            print(f\"Обрезано до {min_samples} образцов\")\n",
    "\n",
    "        data_flattened = data.reshape(data.shape[0], -1)\n",
    "        scaler = StandardScaler()\n",
    "        data_normalized = scaler.fit_transform(data_flattened).reshape(data.shape)\n",
    "\n",
    "        if perform_balancing:\n",
    "            print(f\"Размер до балансировки: {Counter(labels)}\")\n",
    "            rus = RandomUnderSampler(random_state=42)\n",
    "            data_shape = data_normalized.shape\n",
    "            X_reshaped = data_normalized.reshape(data_shape[0], -1)\n",
    "            X_res, y_res = rus.fit_resample(X_reshaped, labels)\n",
    "            data_normalized = X_res.reshape(X_res.shape[0], *data_shape[1:])\n",
    "            labels = y_res\n",
    "            print(f\"Размер после балансировки: {Counter(labels)}\")\n",
    "\n",
    "        return data_normalized, labels\n",
    "\n",
    "    def _predict_torch_model(self, model_name, X):\n",
    "        \"\"\"Внутренний метод для предсказания любой PyTorch модели.\"\"\"\n",
    "        model = self.models.get(model_name)\n",
    "        if model is None: return None\n",
    "\n",
    "        model.eval()\n",
    "        X_tensor = torch.from_numpy(X).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if model_name == \"conformer\":\n",
    "                X_tensor = X_tensor.unsqueeze(1)\n",
    "                _, outputs = model(X_tensor)\n",
    "            elif model_name == \"stmambanet\":\n",
    "                outputs = model(X_tensor, mode=\"classification\")\n",
    "            else: # EEGNet\n",
    "                outputs = model(X_tensor)\n",
    "\n",
    "            preds = torch.argmax(F.softmax(outputs, dim=1), dim=1).cpu().numpy()\n",
    "        return preds\n",
    "\n",
    "    def predict_conformer(self, X): return self._predict_torch_model(\"conformer\", X)\n",
    "    def predict_stmambanet(self, X): return self._predict_torch_model(\"stmambanet\", X)\n",
    "    def predict_eegnet(self, X): return self._predict_torch_model(\"eegnet\", X)\n",
    "\n",
    "    # --- FIX: Simplified training loop without subject IDs ---\n",
    "    def train_eegnet(self, X_train, y_train, X_val, y_val,\n",
    "                     epochs=50, batch_size=32, save_path=None, learning_rate=0.001):\n",
    "        \"\"\"Обучение EEGNet (PyTorch) модели.\"\"\"\n",
    "        model = self.models.get(\"eegnet\")\n",
    "        if model is None: return None\n",
    "\n",
    "        model.to(self.device)\n",
    "        train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "            for data, labels in pbar:\n",
    "                data, labels = data.to(self.device), labels.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if hasattr(model, 'apply_constraints'): model.apply_constraints()\n",
    "\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                pbar.set_postfix({'loss': f'{running_loss/total_train:.4f}', 'acc': f'{correct_train/total_train:.4f}'})\n",
    "\n",
    "            history['loss'].append(running_loss / total_train)\n",
    "            history['accuracy'].append(correct_train / total_train)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, labels in val_loader:\n",
    "                    data, labels = data.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(data)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * data.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_val += labels.size(0)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "            history['val_loss'].append(val_loss / total_val)\n",
    "            history['val_accuracy'].append(correct_val / total_val)\n",
    "            print(f\"Epoch {epoch+1} Summary: Val Loss: {val_loss/total_val:.4f}, Val Acc: {correct_val/total_val:.4f}\")\n",
    "\n",
    "        if save_path:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            self.model_paths[\"eegnet\"] = save_path\n",
    "            print(f\"Модель EEGNet сохранена по пути: {save_path}\")\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Мажоритарное голосование всех доступных моделей.\"\"\"\n",
    "        predictions = []\n",
    "        model_names = []\n",
    "\n",
    "        conformer_preds = self.predict_conformer(X)\n",
    "        if conformer_preds is not None: predictions.append(conformer_preds); model_names.append(\"conformer\")\n",
    "\n",
    "        stmambanet_preds = self.predict_stmambanet(X)\n",
    "        if stmambanet_preds is not None: predictions.append(stmambanet_preds); model_names.append(\"stmambanet\")\n",
    "\n",
    "        eegnet_preds = self.predict_eegnet(X)\n",
    "        if eegnet_preds is not None: predictions.append(eegnet_preds); model_names.append(\"eegnet\")\n",
    "\n",
    "        if not predictions: raise ValueError(\"Нет доступных моделей для прогнозирования\")\n",
    "\n",
    "        stacked_preds = np.stack(predictions, axis=1)\n",
    "        ensemble_preds = []\n",
    "        for i in range(stacked_preds.shape[0]):\n",
    "            votes = stacked_preds[i]\n",
    "            vote_counts = Counter(votes)\n",
    "            most_common = vote_counts.most_common()\n",
    "\n",
    "            if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n",
    "                if \"conformer\" in model_names: ensemble_preds.append(votes[model_names.index(\"conformer\")]); continue\n",
    "                if \"stmambanet\" in model_names: ensemble_preds.append(votes[model_names.index(\"stmambanet\")]); continue\n",
    "                ensemble_preds.append(most_common[0][0])\n",
    "            else:\n",
    "                ensemble_preds.append(most_common[0][0])\n",
    "\n",
    "        return np.array(ensemble_preds)\n",
    "\n",
    "# --- FIX: Main execution block updated for simplified data handling ---\n",
    "if __name__ == \"__main__\":\n",
    "    ensemble = EnsembleModel()\n",
    "    ensemble.load_models()\n",
    "\n",
    "    try:\n",
    "        print(\"\\nЗагрузка и предобработка данных...\")\n",
    "        # Load the full, unbalanced dataset first\n",
    "        X_full, y_full = ensemble.load_and_preprocess_data(CONFIG[\"DATA_PATH_CH\"], CONFIG[\"DATA_PATH\"], perform_balancing=False)\n",
    "        print(f\"Полный набор данных загружен. Форма X: {X_full.shape}, Форма y: {y_full.shape}\")\n",
    "\n",
    "        # Проверяем соответствие размеров перед разбиением\n",
    "        if X_full.shape[0] != y_full.shape[0]:\n",
    "            raise ValueError(f\"Несоответствие размеров между X_full ({X_full.shape[0]}) и y_full ({y_full.shape[0]})\")\n",
    "\n",
    "        # Split the full dataset into training and testing sets\n",
    "        # Stratify by labels to ensure test set is representative of the original data distribution\n",
    "        X_train_unbalanced, X_test, y_train_unbalanced, y_test = train_test_split(\n",
    "            X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)\n",
    "\n",
    "        print(f\"Размер тестового набора: {X_test.shape}, Распределение: {Counter(y_test)}\")\n",
    "\n",
    "        # Now, balance ONLY the training data\n",
    "        print(\"\\nБалансировка только обучающего набора...\")\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_train_reshaped = X_train_unbalanced.reshape(X_train_unbalanced.shape[0], -1)\n",
    "        X_train_balanced_reshaped, y_train_balanced = rus.fit_resample(X_train_reshaped, y_train_unbalanced)\n",
    "        X_train = X_train_balanced_reshaped.reshape(X_train_balanced_reshaped.shape[0], *X_train_unbalanced.shape[1:])\n",
    "        y_train = y_train_balanced\n",
    "\n",
    "        print(f\"Размер обучающего набора после балансировки: {X_train.shape}, Распределение: {Counter(y_train)}\")\n",
    "\n",
    "        if ensemble.model_paths[\"eegnet\"] is None:\n",
    "            print(\"\\nОбучение модели EEGNet...\")\n",
    "            X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "                X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "            history = ensemble.train_eegnet(\n",
    "                X_train_split, y_train_split,\n",
    "                X_val, y_val,\n",
    "                epochs=10, batch_size=64,\n",
    "                save_path='/content/drive/MyDrive/Colab Notebooks/Ensemble_Adlet/trained_eegnet_generalized.pt'\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                import matplotlib.pyplot as plt\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.subplot(1, 2, 1); plt.plot(history['loss'], label='train'); plt.plot(history['val_loss'], label='validation'); plt.title('Функция потерь'); plt.legend()\n",
    "                plt.subplot(1, 2, 2); plt.plot(history['accuracy'], label='train'); plt.plot(history['val_accuracy'], label='validation'); plt.title('Точность'); plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('/content/drive/MyDrive/Colab Notebooks/Ensemble_Adlet/eegnet_training_history_generalized.png')\n",
    "                print(\"График истории обучения сохранен\")\n",
    "            except Exception as e: print(f\"Не удалось создать график истории обучения: {e}\")\n",
    "\n",
    "        print(\"\\nОценка каждой модели отдельно на тестовом наборе:\")\n",
    "        for name in [\"conformer\", \"stmambanet\", \"eegnet\"]:\n",
    "            if ensemble.models.get(name):\n",
    "                preds = ensemble._predict_torch_model(name, X_test)\n",
    "                acc = np.mean(preds == y_test)\n",
    "                print(f\"Точность {name.capitalize()}: {acc:.4f}\")\n",
    "\n",
    "        print(\"\\nОценка ансамблевой модели:\")\n",
    "        predictions = ensemble.predict(X_test)\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        print(f\"Точность ансамбля: {accuracy:.4f}\")\n",
    "\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        print(\"\\nМатрица ошибок:\")\n",
    "        print(confusion_matrix(y_test, predictions))\n",
    "        print(\"\\nОтчет о классификации:\")\n",
    "        print(classification_report(y_test, predictions, target_names=[\"Класс 0\", \"Класс 1\"]))\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Произошла ошибка: {e}\")\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nИспользование случайных данных для демонстрации...\")\n",
    "        X_demo = np.random.randn(10, 55, 100).astype(np.float32)\n",
    "        predictions = ensemble.predict(X_demo)\n",
    "        print(\"Ансамблевые предсказания (случайные данные):\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCyy-3IGrTAF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4c1ca39a"
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-build-isolation mamba-ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1753181672292,
     "user": {
      "displayName": "Ilyas Nagiyev",
      "userId": "14931678141910690558"
     },
     "user_tz": -300
    },
    "id": "3f52ece4",
    "outputId": "6be7cf3e-a906-4f63-bbab-fee00d5576dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Сodes.zip', 'best_conformer_with_freq.pt', 'compare_classes.txt', 'ensemble_model.py', '.DS_Store', 'EEG-conformer.ipynb', 'conformer.py', 'BalancedAcrossSubjectsAllData.ipynb', 'data', 'data_ch', '__pycache__', 'STmambaNet_the_last_version_2.ipynb', 'best_model_mamba.pt', 'best_conformer_default.pt', 'Preprocessing.png', 'trained_eegnet_generalized.pt', 'eegnet_training_history_generalized.png', 'conformer_opt.py', 'Preprocess_freq.py', 'STmambaNet_the_last_version.ipynb', 'Untitled0.ipynb', 'Adlet_ensemble.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/content/drive/MyDrive/Colab_Notebooks/Ensemble_Adlet/'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
